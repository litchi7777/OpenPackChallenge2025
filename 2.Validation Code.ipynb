{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 7th Factory Work Activity Recognition Challenge\n",
    "\n",
    "This notebook has been designed for the 7th Factory Work Activity Recognition Challenge competition with the aim of Activity Recognition using REAL Accelerometer from OpenPack dataset and GENERATED Accelerometer created by subjects.\n",
    "\n",
    "If you have any questions, please feel free to email abc2025@sozolab.jp with the subject 7th Factory Work Activity Recognition Challenge.\n",
    "\n",
    "About this dataset and challenge -> https://abc-research.github.io/challenge2025/\n",
    "\n",
    "This notebook was prepared by Qingxin Xia."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f251e07c11db0713"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mount Drive\n",
    "\n",
    "This tutorial is made in Google Colab. So, first we need to connect the Google Drive to access the data. You can directly add folder path to access the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d843c988b9ef7b0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-25T05:26:29.294878400Z",
     "start_time": "2024-11-25T05:26:29.291889Z"
    }
   },
   "id": "943f2d92923662a1",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Necessary Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adde54465537d1b3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# from transforms3d.axangles import axangle2mat  # for rotation\n",
    "from scipy.interpolate import CubicSpline  # for warping\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T17:40:37.951189Z",
     "start_time": "2024-11-26T17:40:35.164127100Z"
    }
   },
   "id": "a2574b92f4441c5d",
   "execution_count": 247
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T17:32:35.657476100Z",
     "start_time": "2024-11-26T17:32:35.555296600Z"
    }
   },
   "id": "71e4e4342fba656c",
   "execution_count": 241
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "splits = [0.7, 0.1, 0.2]\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    # Set seed for Python's random module\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Set seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set seed for PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # If using CUDA, set seed for GPU as well\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "\n",
    "# Set a fixed random seed\n",
    "seed_value = 42\n",
    "set_random_seed(seed_value)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f2597443fadcb88"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare data\n",
    "\n",
    "https://github.com/open-pack/openpack-dataset/blob/main/docs/DOWNLOAD.md"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de408a3c7e64de0d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Folders\n",
    "\n",
    "Firstly, a folder is created to save OpenPack dataset: '/data/real/'.\n",
    "\n",
    "Subjects are required to download OpenPack dataset by themselves. The data should be placed at: '/data/real/'.\n",
    "\n",
    "Another folder '/data/virtual/' will be created to save generated data.\n",
    " *(Pay attention, the size of this folder is limited to TBD.)*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af2caace48e5668f"
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-26T15:23:45.250297800Z",
     "start_time": "2024-11-26T15:23:45.119289600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '\\data\\real' created successfully.\n",
      "Directory '\\data\\virtual' created successfully.\n"
     ]
    }
   ],
   "source": [
    "realpath = r'\\data\\real'\n",
    "\n",
    "# Create the directory\n",
    "os.makedirs(realpath, exist_ok=True)\n",
    "\n",
    "print(f\"Directory '{realpath}' created successfully.\")\n",
    "\n",
    "virtpath = r'\\data\\virtual'\n",
    "\n",
    "# Create the directory\n",
    "os.makedirs(virtpath, exist_ok=True)\n",
    "\n",
    "print(f\"Directory '{virtpath}' created successfully.\")\n",
    "\n",
    "rootdir = r'D:\\code\\OpenPackChallenge2025'  # replace with your project path\n",
    "real_directory = rootdir + realpath  \n",
    "virt_directory = rootdir + virtpath  \n",
    "# directory = r'D:\\code\\OpenPackChallenge2025\\data\\real'  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Unzip OpenPack dataset\n",
    "\n",
    "After placing the OpenPack dataset at the '/data/real/' folder, unzip the files and delete the zip files."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30a7941d4d94b239"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted: imu-with-operation-action-labels.zip\n",
      "Deleted: imu-with-operation-action-labels.zip\n",
      "All zip files have been processed.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(real_directory):\n",
    "    # Construct full file path\n",
    "    file_path = os.path.join(real_directory, filename)\n",
    "    \n",
    "    # Check if the file is a zip file\n",
    "    if filename.endswith('.zip'):\n",
    "        # Open the zip file\n",
    "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "            # Extract all contents of the zip file into the directory\n",
    "            zip_ref.extractall(file_path[:-4])\n",
    "            print(f\"Extracted: {filename}\")\n",
    "        \n",
    "        # Delete the zip file\n",
    "        os.remove(file_path)\n",
    "        print(f\"Deleted: {filename}\")\n",
    "\n",
    "print(\"All zip files have been processed.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T11:40:11.083311800Z",
     "start_time": "2024-11-26T11:40:00.694264800Z"
    }
   },
   "id": "bd9a758fa4d9d166",
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T07:44:36.991258900Z",
     "start_time": "2024-11-26T07:44:36.965684100Z"
    }
   },
   "id": "c649ddea7bc19d21",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use Real Data to Generate Virtual Data\n",
    "\n",
    "First, we will randomly split the real data into a training set, validation set, and test set according to a certain ratio. Then, we will use the training set data to generate virtual data. Finally, we will train the network using the training set and virtual data, and calculate the F1 score on the test set.\n",
    "\n",
    "The following code is an example. Note: (1) The model structure is fixed and unchanged. (2) The split ratio for the training and test sets and the random seed will not be disclosed. (3) Subjects are free to design data generation algorithms and save them to a specified path: '/data/virtual/'."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a5a8f3aead7a511"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assign train users, validation users, and test users\n",
    "\n",
    "In the OpenPack dataset, U0xxx corresponds to user IDs, and S0xxx corresponds to different experiment settings.\n",
    "\n",
    "In this Challenge, we will only select training (real) and test data from S0100."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba365afde03f932"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filter out un-used data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b92d0517e92b247c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U0101 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0101-S0100.csv\n",
      "U0102 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0102-S0100.csv\n",
      "U0103 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0103-S0100.csv\n",
      "U0104 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0104-S0100.csv\n",
      "U0105 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0105-S0100.csv\n",
      "U0106 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0106-S0100.csv\n",
      "U0107 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0107-S0100.csv\n",
      "U0108 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0108-S0100.csv\n",
      "U0109 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0109-S0100.csv\n",
      "U0110 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0110-S0100.csv\n",
      "U0111 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0111-S0100.csv\n",
      "U0201 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0201-S0100.csv\n",
      "U0202 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0202-S0100.csv\n",
      "U0203 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0203-S0100.csv\n",
      "U0204 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0204-S0100.csv\n",
      "U0205 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0205-S0100.csv\n",
      "U0206 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0206-S0100.csv\n",
      "U0207 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0207-S0100.csv\n",
      "U0208 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0208-S0100.csv\n",
      "U0209 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0209-S0100.csv\n",
      "U0210 at: D:\\code\\OpenPackChallenge2025\\data\\real\\imu-with-operation-action-labels\\imu-with-operation-action-labels\\U0210-S0100.csv\n"
     ]
    }
   ],
   "source": [
    "user_paths = {}\n",
    "for root, dirs, files in os.walk(real_directory):\n",
    "    for file in files:\n",
    "        if file.endswith('S0100.csv'):\n",
    "            user_paths[file[:-10]] = os.path.join(root, file)\n",
    "for u, d in user_paths.items():\n",
    "    print('%s at: %s'% (u,d))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T11:58:21.525546600Z",
     "start_time": "2024-11-26T11:58:21.495067600Z"
    }
   },
   "id": "c071d8483f0a0c41",
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e088a3db85d2fb82"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split users to train, validation, and test sets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdd3fbfea9d0344"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: ['U0103' 'U0104' 'U0105' 'U0106' 'U0107' 'U0109' 'U0110' 'U0201' 'U0203'\n",
      " 'U0204' 'U0206' 'U0208' 'U0209' 'U0210']\n",
      "Validation set: ['U0102' 'U0202']\n",
      "Test set: ['U0101' 'U0108' 'U0111' 'U0205' 'U0207']\n"
     ]
    }
   ],
   "source": [
    "# splits = [0.7, 0.1, 0.2]\n",
    "\n",
    "userIDs = list(user_paths.keys())\n",
    "\n",
    "# Shuffle the list to ensure randomness\n",
    "random.shuffle(userIDs)\n",
    "\n",
    "# Calculate the split indices\n",
    "total_length = len(userIDs)\n",
    "train_size = int(total_length * splits[0])  # 70% of 10\n",
    "val_size = int(total_length * splits[1])  # 10% of 10\n",
    "test_size = total_length - train_size - val_size  # 20% of 10\n",
    "\n",
    "# Split the list according to the calculated sizes\n",
    "train_users = np.sort(userIDs[:train_size])      # First 70%\n",
    "val_users = np.sort(userIDs[train_size:train_size + val_size])  # Next 10%\n",
    "test_users = np.sort(userIDs[train_size + val_size:])  # Last 20%\n",
    "\n",
    "print('Training set: %s'%train_users)\n",
    "print('Validation set: %s'%val_users)\n",
    "print('Test set: %s'%test_users)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T12:16:52.484542700Z",
     "start_time": "2024-11-26T12:16:52.450090200Z"
    }
   },
   "id": "b9df5a7359851e8c",
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3a1eae40d1cbe3a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data according to userIDs\n",
    "\n",
    "Load data of every user as dataframe.\n",
    "Use acceleration data of both wrists only;\n",
    "Use operation label."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1de07e6c2e80afb9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "selected_columns = ['atr01/acc_x','atr01/acc_y','atr01/acc_z','atr02/acc_x','atr02/acc_y','atr02/acc_z','timestamp','operation'] \n",
    "train_data_dict = {}\n",
    "for u in train_users:\n",
    "    # Load the CSV file with only the selected columns\n",
    "    train_data_dict[u] = pd.read_csv(user_paths[u], usecols=selected_columns)\n",
    "\n",
    "val_data_dict = {}\n",
    "for u in val_users:\n",
    "    # Load the CSV file with only the selected columns\n",
    "    val_data_dict[u] = pd.read_csv(user_paths[u], usecols=selected_columns)\n",
    "    \n",
    "test_data_dict = {}\n",
    "for u in test_users:\n",
    "    # Load the CSV file with only the selected columns\n",
    "    test_data_dict[u] = pd.read_csv(user_paths[u], usecols=selected_columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T12:34:50.434498700Z",
     "start_time": "2024-11-26T12:34:45.441242400Z"
    }
   },
   "id": "cfbcc63914ccaf29",
   "execution_count": 86
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implement data augmentation with training data and save them to folder\n",
    "\n",
    "Please save the generated data as csv file in '/data/virtual/'."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "908ae10f54d3cfd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code implements a list of transforms for tri-axial raw-accelerometry\n",
    "We assume that the input format is of size:\n",
    "3 x (epoch_len * sampling_frequency)\n",
    "\n",
    "Transformations included:\n",
    "1. jitter\n",
    "2. Channel shuffling: which axis is being switched\n",
    "3. Horizontal flip: binary\n",
    "4. Permutation: binary\n",
    "\n",
    "This script is mostly based off from\n",
    "https://github.com/terryum/Data-Augmentation-For-Wearable-Sensor-Data/blob/master/Example_DataAugmentation_TimeseriesData.py\n",
    "\"\"\"\n",
    "\n",
    "def switch_axis(sample, choice):\n",
    "    \"\"\"\n",
    "    Randomly switch the three axises for the raw files\n",
    "\n",
    "    Args:\n",
    "        sample (numpy array): 3 * FEATURE_SIZE\n",
    "        choice (int): 0-6 for direction selection\n",
    "    \"\"\"\n",
    "    x = sample[0, :]\n",
    "    y = sample[1, :]\n",
    "    z = sample[2, :]\n",
    "\n",
    "    if choice == 0:\n",
    "        return sample\n",
    "    elif choice == 1:\n",
    "        sample = np.stack([x, y, z], axis=0)\n",
    "    elif choice == 2:\n",
    "        sample = np.stack([x, z, y], axis=0)\n",
    "    elif choice == 3:\n",
    "        sample = np.stack([y, x, z], axis=0)\n",
    "    elif choice == 4:\n",
    "        sample = np.stack([y, z, x], axis=0)\n",
    "    elif choice == 5:\n",
    "        sample = np.stack([z, x, y], axis=0)\n",
    "    elif choice == 6:\n",
    "        sample = np.stack([z, y, x], axis=0)\n",
    "    return sample\n",
    "\n",
    "\n",
    "def flip(sample, choice):\n",
    "    \"\"\"\n",
    "    Flip over the actigram on the temporal scale\n",
    "\n",
    "    Args:\n",
    "        sample (numpy array): 3 * FEATURE_SIZE\n",
    "        choice (int): 0-1 binary\n",
    "    \"\"\"\n",
    "    if choice == 1:\n",
    "        sample = np.flip(sample, 1)\n",
    "    return sample\n",
    "\n",
    "\n",
    "def DA_Permutation(X, nPerm=4, minSegLength=10):\n",
    "    X_new = np.zeros(X.shape)\n",
    "    idx = np.random.permutation(nPerm)\n",
    "    bWhile = True\n",
    "    while bWhile is True:\n",
    "        segs = np.zeros(nPerm + 1, dtype=int)\n",
    "        segs[1:-1] = np.sort(\n",
    "            np.random.randint(\n",
    "                minSegLength, X.shape[0] - minSegLength, nPerm - 1\n",
    "            )\n",
    "        )\n",
    "        segs[-1] = X.shape[0]\n",
    "        if np.min(segs[1:] - segs[0:-1]) > minSegLength:\n",
    "            bWhile = False\n",
    "    pp = 0\n",
    "    for ii in range(nPerm):\n",
    "        x_temp = X[segs[idx[ii]] : segs[idx[ii] + 1], :]\n",
    "        X_new[pp : pp + len(x_temp), :] = x_temp\n",
    "        pp += len(x_temp)\n",
    "    return X_new\n",
    "\n",
    "\n",
    "def permute(sample, choice, nPerm=4, minSegLength=10):\n",
    "    \"\"\"\n",
    "    Distort an epoch by dividing up the sample into several segments and\n",
    "    then permute them\n",
    "\n",
    "    Args:\n",
    "        sample (numpy array): 3 * FEATURE_SIZE\n",
    "        choice (int): 0-1 binary\n",
    "    \"\"\"\n",
    "    if choice == 1:\n",
    "        sample = np.swapaxes(sample, 0, 1)\n",
    "        sample = DA_Permutation(sample, nPerm=nPerm, minSegLength=minSegLength)\n",
    "        sample = np.swapaxes(sample, 0, 1)\n",
    "    return sample\n",
    "\n",
    "\n",
    "def is_scaling_factor_invalid(scaling_factor, min_scale_sigma):\n",
    "    \"\"\"\n",
    "    Ensure each of the abs values of the scaling\n",
    "    factors are greater than the min\n",
    "    \"\"\"\n",
    "    for i in range(len(scaling_factor)):\n",
    "        if abs(scaling_factor[i] - 1) < min_scale_sigma:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def DA_Scaling(X, sigma=0.3, min_scale_sigma=0.05):\n",
    "    scaling_factor = np.random.normal(\n",
    "        loc=1.0, scale=sigma, size=(1, X.shape[1])\n",
    "    )  # shape=(1,3)\n",
    "    while is_scaling_factor_invalid(scaling_factor, min_scale_sigma):\n",
    "        scaling_factor = np.random.normal(\n",
    "            loc=1.0, scale=sigma, size=(1, X.shape[1])\n",
    "        )\n",
    "    my_noise = np.matmul(np.ones((X.shape[0], 1)), scaling_factor)\n",
    "    X = X * my_noise\n",
    "    return X\n",
    "\n",
    "\n",
    "def scaling_uniform(X, scale_range=0.15, min_scale_diff=0.02):\n",
    "    low = 1 - scale_range\n",
    "    high = 1 + scale_range\n",
    "    scaling_factor = np.random.uniform(\n",
    "        low=low, high=high, size=(X.shape[1])\n",
    "    )  # shape=(3)\n",
    "    while is_scaling_factor_invalid(scaling_factor, min_scale_diff):\n",
    "        scaling_factor = np.random.uniform(\n",
    "            low=low, high=high, size=(X.shape[1])\n",
    "        )\n",
    "\n",
    "    for i in range(3):\n",
    "        X[:, i] = X[:, i] * scaling_factor[i]\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def scale(sample, choice, scale_range=0.5, min_scale_diff=0.15):\n",
    "    if choice == 1:\n",
    "        sample = np.swapaxes(sample, 0, 1)\n",
    "        sample = scaling_uniform(\n",
    "            sample, scale_range=scale_range, min_scale_diff=min_scale_diff\n",
    "        )\n",
    "        sample = np.swapaxes(sample, 0, 1)\n",
    "    return sample\n",
    "\n",
    "\n",
    "def DistortTimesteps(X, sigma=0.2):\n",
    "    tt = GenerateRandomCurves(\n",
    "        X, sigma\n",
    "    )  # Regard these samples aroun 1 as time intervals\n",
    "    tt_cum = np.cumsum(tt, axis=0)  # Add intervals to make a cumulative graph\n",
    "    # Make the last value to have X.shape[0]\n",
    "    t_scale = [\n",
    "        (X.shape[0] - 1) / tt_cum[-1, 0],\n",
    "        (X.shape[0] - 1) / tt_cum[-1, 1],\n",
    "        (X.shape[0] - 1) / tt_cum[-1, 2],\n",
    "    ]\n",
    "    tt_cum[:, 0] = tt_cum[:, 0] * t_scale[0]\n",
    "    tt_cum[:, 1] = tt_cum[:, 1] * t_scale[1]\n",
    "    tt_cum[:, 2] = tt_cum[:, 2] * t_scale[2]\n",
    "    return tt_cum\n",
    "\n",
    "\n",
    "def GenerateRandomCurves(X, sigma=0.2, knot=4):\n",
    "    xx = (\n",
    "        np.ones((X.shape[1], 1))\n",
    "        * (np.arange(0, X.shape[0], (X.shape[0] - 1) / (knot + 1)))\n",
    "    ).transpose()\n",
    "    yy = np.random.normal(loc=1.0, scale=sigma, size=(knot + 2, X.shape[1]))\n",
    "    x_range = np.arange(X.shape[0])\n",
    "    cs_x = CubicSpline(xx[:, 0], yy[:, 0])\n",
    "    cs_y = CubicSpline(xx[:, 1], yy[:, 1])\n",
    "    cs_z = CubicSpline(xx[:, 2], yy[:, 2])\n",
    "    return np.array([cs_x(x_range), cs_y(x_range), cs_z(x_range)]).transpose()\n",
    "\n",
    "\n",
    "def DA_TimeWarp(X, sigma=0.2):\n",
    "    tt_new = DistortTimesteps(X, sigma)\n",
    "    X_new = np.zeros(X.shape)\n",
    "    x_range = np.arange(X.shape[0])\n",
    "    X_new[:, 0] = np.interp(x_range, tt_new[:, 0], X[:, 0])\n",
    "    X_new[:, 1] = np.interp(x_range, tt_new[:, 1], X[:, 1])\n",
    "    X_new[:, 2] = np.interp(x_range, tt_new[:, 2], X[:, 2])\n",
    "    return X_new\n",
    "\n",
    "\n",
    "def time_warp(sample, choice, sigma=0.2):\n",
    "    if choice == 1:\n",
    "        sample = np.swapaxes(sample, 0, 1)\n",
    "        sample = DA_TimeWarp(sample, sigma=sigma)\n",
    "        sample = np.swapaxes(sample, 0, 1)\n",
    "    return sample\n",
    "\n",
    "def custom_virtual_data_generation(data):\n",
    "    '''\n",
    "    Please modify the code and submit this function and its relative functions.\n",
    "    :param data: numpy array, shape is (data length, dim=6)\n",
    "    :return: numpy array, shape is (data length, dim=6)\n",
    "    '''\n",
    "     # Data augmentations\n",
    "    left = permute(data[:,:3].transpose(), 1)\n",
    "    right = time_warp(data[:,3:].transpose(), 1)\n",
    "    \n",
    "    new_data = np.concatenate([left.transpose(), right.transpose()], axis=1)\n",
    "    return new_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25a44a5f3977b74"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating virtual data from user U0103.\n",
      "Generating virtual data from user U0104.\n",
      "Generating virtual data from user U0105.\n",
      "Generating virtual data from user U0106.\n",
      "Generating virtual data from user U0107.\n",
      "Generating virtual data from user U0109.\n",
      "Generating virtual data from user U0110.\n",
      "Generating virtual data from user U0201.\n",
      "Generating virtual data from user U0203.\n",
      "Generating virtual data from user U0204.\n",
      "Generating virtual data from user U0206.\n",
      "Generating virtual data from user U0208.\n",
      "Generating virtual data from user U0209.\n",
      "Generating virtual data from user U0210.\n"
     ]
    }
   ],
   "source": [
    "# virtual_data_path = r'D:\\code\\OpenPackChallenge2025\\data\\virtual'\n",
    "\n",
    "new_columns = selected_columns[:6] + [selected_columns[-1]]\n",
    "for u, df in train_data_dict.items():\n",
    "    print('Generating virtual data from user %s.'% u)\n",
    "    # Extract sensor data and labels\n",
    "    raw_data = df[selected_columns[:6]].values\n",
    "    labels = df[selected_columns[-1]].values.reshape(-1,1)\n",
    "    \n",
    "    tmp = custom_virtual_data_generation(raw_data)\n",
    "    \n",
    "    # Concatenate data with operation labels\n",
    "    virtual_data = np.concatenate([tmp, labels], axis=1)\n",
    "    \n",
    "    # Convert np.array to dataframe\n",
    "    df = pd.DataFrame(virtual_data, columns=new_columns)\n",
    "    \n",
    "    # Save data to /data/virtual/\n",
    "    df.to_csv(os.path.join(virt_directory, u+'.csv'), index=False) \n",
    "    # break\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T17:05:39.642221500Z",
     "start_time": "2024-11-26T17:05:30.021475200Z"
    }
   },
   "id": "d86474abe4b28060",
   "execution_count": 187
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cb41e3073faa5f67"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use the Generated Data to Improve HAR Model Performance\n",
    "\n",
    "Do not need to change the following code."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c18c2a90cf62034"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read data and labels from both real and virtual folders"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6356dfab9eb38848"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for train, validation, and test: ['atr01/acc_x', 'atr01/acc_y', 'atr01/acc_z', 'atr02/acc_x', 'atr02/acc_y', 'atr02/acc_z', 'operation']\n"
     ]
    }
   ],
   "source": [
    "# only select data and label columns\n",
    "new_columns = selected_columns[:6] + [selected_columns[-1]]\n",
    "print('Data for train, validation, and test: %s'%new_columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T17:06:13.569694200Z",
     "start_time": "2024-11-26T17:06:13.463967600Z"
    }
   },
   "id": "6c281787d35f62f6",
   "execution_count": 189
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virtual csv file paths are as shown follows:\n"
     ]
    },
    {
     "data": {
      "text/plain": "['D:\\\\code\\\\OpenPackChallenge2025\\\\data\\\\virtual\\\\U0103.csv',\n 'D:\\\\code\\\\OpenPackChallenge2025\\\\data\\\\virtual\\\\U0104.csv',\n 'D:\\\\code\\\\OpenPackChallenge2025\\\\data\\\\virtual\\\\U0105.csv',\n 'D:\\\\code\\\\OpenPackChallenge2025\\\\data\\\\virtual\\\\U0106.csv',\n 'D:\\\\code\\\\OpenPackChallenge2025\\\\data\\\\virtual\\\\U0107.csv',\n 'D:\\\\code\\\\OpenPackChallenge2025\\\\data\\\\virtual\\\\U0109.csv',\n 'D:\\\\code\\\\OpenPackChallenge2025\\\\data\\\\virtual\\\\U0110.csv',\n 'D:\\\\code\\\\OpenPackChallenge2025\\\\data\\\\virtual\\\\U0201.csv',\n 'D:\\\\code\\\\OpenPackChallenge2025\\\\data\\\\virtual\\\\U0203.csv',\n 'D:\\\\code\\\\OpenPackChallenge2025\\\\data\\\\virtual\\\\U0204.csv',\n 'D:\\\\code\\\\OpenPackChallenge2025\\\\data\\\\virtual\\\\U0206.csv',\n 'D:\\\\code\\\\OpenPackChallenge2025\\\\data\\\\virtual\\\\U0208.csv',\n 'D:\\\\code\\\\OpenPackChallenge2025\\\\data\\\\virtual\\\\U0209.csv',\n 'D:\\\\code\\\\OpenPackChallenge2025\\\\data\\\\virtual\\\\U0210.csv']"
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find csv files in 'data/virtual'\n",
    "virt_paths = []\n",
    "for root, dirs, files in os.walk(virt_directory):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            virt_paths.append(os.path.join(root, file))\n",
    "print('Virtual csv file paths are as shown follows:')\n",
    "virt_paths"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T17:06:15.505458200Z",
     "start_time": "2024-11-26T17:06:15.440409700Z"
    }
   },
   "id": "b52a01881aa3e839",
   "execution_count": 190
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data is (1932072, 7)\n"
     ]
    }
   ],
   "source": [
    "# real and virtual training data\n",
    "\n",
    "## real data\n",
    "train_data = []\n",
    "for u, data in train_data_dict.items():\n",
    "    train_data.append(data[new_columns].values)\n",
    "    # print(data[new_columns].values.shape)\n",
    "    \n",
    "## virtual data\n",
    "for p in virt_paths:\n",
    "    # Load the CSV file with only the selected columns\n",
    "    data = pd.read_csv(p, usecols=new_columns)\n",
    "    train_data.append(data.values)\n",
    "\n",
    "train_data = np.concatenate(train_data, axis=0)\n",
    "print('Shape of train data is %s'%str(train_data.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T17:06:31.374298Z",
     "start_time": "2024-11-26T17:06:29.371508900Z"
    }
   },
   "id": "317fd3932804be3e",
   "execution_count": 192
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation data is (165963, 7)\n",
      "Shape of test data is (357219, 7)\n"
     ]
    }
   ],
   "source": [
    "# validatation and test data\n",
    "val_data = []\n",
    "for u, data in val_data_dict.items():\n",
    "    val_data.append(data[new_columns].values)\n",
    "    \n",
    "test_data = []\n",
    "for u, data in test_data_dict.items():\n",
    "    test_data.append(data[new_columns].values)\n",
    "\n",
    "val_data = np.concatenate(val_data, axis=0)\n",
    "test_data = np.concatenate(test_data, axis=0)\n",
    "\n",
    "print('Shape of validation data is %s'%str(val_data.shape))\n",
    "print('Shape of test data is %s'%str(test_data.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T17:07:26.403366900Z",
     "start_time": "2024-11-26T17:07:26.246585200Z"
    }
   },
   "id": "ad48585ae2ea52d6",
   "execution_count": 197
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "8100.0",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[205], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m label_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(labels, np\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;28mlen\u001B[39m(labels))))\n\u001B[0;32m      4\u001B[0m train_data[:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([label_dict[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m train_data[:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]])\n\u001B[1;32m----> 5\u001B[0m val_data[:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m  np\u001B[38;5;241m.\u001B[39marray([label_dict[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m val_data[:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]])\n\u001B[0;32m      6\u001B[0m test_data[:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m  np\u001B[38;5;241m.\u001B[39marray([label_dict[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m test_data[:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]])\n",
      "Cell \u001B[1;32mIn[205], line 5\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      3\u001B[0m label_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(labels, np\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;28mlen\u001B[39m(labels))))\n\u001B[0;32m      4\u001B[0m train_data[:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([label_dict[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m train_data[:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]])\n\u001B[1;32m----> 5\u001B[0m val_data[:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m  np\u001B[38;5;241m.\u001B[39marray([label_dict[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m val_data[:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]])\n\u001B[0;32m      6\u001B[0m test_data[:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m  np\u001B[38;5;241m.\u001B[39marray([label_dict[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m test_data[:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]])\n",
      "\u001B[1;31mKeyError\u001B[0m: 8100.0"
     ]
    }
   ],
   "source": [
    "# convert operation ID to labels (from 0 to n)\n",
    "labels = np.unique(train_data[:, -1])\n",
    "label_dict = dict(zip(labels, np.arange(len(labels))))\n",
    "train_data[:,-1] = np.array([label_dict[i] for i in train_data[:,-1]])\n",
    "val_data[:,-1] =  np.array([label_dict[i] for i in val_data[:,-1]])\n",
    "test_data[:,-1] =  np.array([label_dict[i] for i in test_data[:,-1]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T17:09:38.449495900Z",
     "start_time": "2024-11-26T17:09:37.884790900Z"
    }
   },
   "id": "5853fe098d9073b0",
   "execution_count": 205
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a4f7ecdf96bd4fff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Dataloader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9693fd76fa677383"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class data_loader_OpenPack(Dataset):\n",
    "    def __init__(self, samples, labels, device='cpu'):\n",
    "        self.samples = torch.tensor(samples).to(device)  # check data type\n",
    "        self.labels = torch.tensor(labels)  # check data type\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        target = self.labels[index]\n",
    "        sample = self.samples[index]\n",
    "        return sample, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "def sliding_window(datanp, len_sw, step):\n",
    "    '''\n",
    "    :param datanp: shape=(data length, dim) raw sensor data and the labels. The last column is the label column.\n",
    "    :param len_sw: length of the segmented sensor data \n",
    "    :param step: overlapping length of the segmented data \n",
    "    :return: shape=(N, len_sw, dim) batch of sensor data segment.\n",
    "    '''\n",
    "    \n",
    "    # generate batch of data by overlapping the training set\n",
    "    data_batch = []\n",
    "    for idx in range(0, datanp.shape[0] - len_sw - step, step):  \n",
    "        data_batch.append(datanp[idx: idx + len_sw, :])\n",
    "    data_batch.append(datanp[-1 - len_sw: -1, :])  # last batch\n",
    "    xlist = np.stack(data_batch, axis=0)  # [B, data length, dim]\n",
    "\n",
    "    return xlist\n",
    "\n",
    "def generate_dataloader(data, len_sw, step, if_shuffle=True):\n",
    "    tmp_b = sliding_window(data, len_sw, step)\n",
    "    data_b = tmp_b[:, :, :-1]\n",
    "    label_b = tmp_b[:, :, -1] \n",
    "    data_set_r = data_loader_OpenPack(data_b, label_b, device=device)\n",
    "    data_loader = DataLoader(data_set_r, batch_size=batch_size,\n",
    "                              shuffle=if_shuffle, drop_last=False)\n",
    "    return data_loader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T17:10:20.929443300Z",
     "start_time": "2024-11-26T17:10:20.898518300Z"
    }
   },
   "id": "5941002ccfa02913",
   "execution_count": 208
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len_sw = 600\n",
    "step = 300\n",
    "batch_size = 512\n",
    "\n",
    "train_loader = generate_dataloader(train_data, len_sw, step, if_shuffle=True)\n",
    "val_loader = generate_dataloader(val_data, len_sw, step, if_shuffle=False)\n",
    "test_loader = generate_dataloader(test_data, len_sw, step, if_shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T17:10:24.113448400Z",
     "start_time": "2024-11-26T17:10:23.094960500Z"
    }
   },
   "id": "d7d6e62da61c17d9",
   "execution_count": 210
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T17:10:24.897616800Z",
     "start_time": "2024-11-26T17:10:24.881966Z"
    }
   },
   "id": "6eb8b0f180534213",
   "execution_count": 210
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Model\n",
    "\n",
    "Reference:\n",
    "https://github.com/open-pack/openpack-torch/blob/main/openpack_torch/models/imu/deep_conv_lstm.py#L78"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be50fc0f4fdbbb16"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DeepConvLSTMSelfAttn(nn.Module):\n",
    "    \"\"\"Imprementation of a DeepConvLSTM with Self-Attention used in ''Deep ConvLSTM with\n",
    "    self-attention for human activity decoding using wearable sensors'' (Sensors 2020).\n",
    "\n",
    "    Note:\n",
    "        https://ieeexplore.ieee.org/document/9296308 (Sensors 2020)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch: int = 6,\n",
    "        num_classes: int = None,\n",
    "        cnn_filters=3,\n",
    "        lstm_units=32,\n",
    "        num_attn_heads: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # NOTE: The first block is input layer.\n",
    "\n",
    "        # -- [1] Embedding Layer --\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, cnn_filters, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(cnn_filters),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # -- [2] LSTM Encoder --\n",
    "        self.lstm = nn.LSTM(cnn_filters, lstm_units, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        # -- [3] Self-Attention --\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            lstm_units,\n",
    "            num_attn_heads,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # -- [4] Softmax Layer (Output Layer) --\n",
    "        self.out = nn.Conv2d(\n",
    "            lstm_units,\n",
    "            num_classes,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): shape = (B, T, CH)\n",
    "        Returns:\n",
    "            torch.Tensor: shape = (B, N_CLASSES, T)\n",
    "        \"\"\"\n",
    "        # -- [0] Convert Input Shape --\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = x.unsqueeze(3)  # output shape = (B, CH, T, 1)\n",
    "        \n",
    "        # -- [1] Embedding Layer --\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # -- [2] LSTM Encoder --\n",
    "        # Reshape: (B, CH, 1, T) -> (B, T, CH)\n",
    "        x = x.squeeze(3).transpose(1, 2)\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # -- [3] Self-Attention --\n",
    "        x, w = self.attention(x.clone(), x.clone(), x.clone())\n",
    "\n",
    "        # -- [4] Softmax Layer (Output Layer) --\n",
    "        # Reshape: (B, T, CH) -> (B, CH, T, 1)\n",
    "        x = x.transpose(1, 2).unsqueeze(3)\n",
    "        x = self.out(x)\n",
    "        x = x.squeeze(3)\n",
    "        return x  # (B, N_CLASSES, T)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T17:31:54.694552300Z",
     "start_time": "2024-11-26T17:31:54.672758900Z"
    }
   },
   "id": "c655b4c595520ad2",
   "execution_count": 239
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepConvLSTMSelfAttn(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(6, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (lstm): LSTM(3, 32, batch_first=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "  )\n",
      "  (out): Conv2d(32, 11, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = DeepConvLSTMSelfAttn(num_classes=len(label_dict))\n",
    "model = model.to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T17:32:47.180838500Z",
     "start_time": "2024-11-26T17:32:47.100693300Z"
    }
   },
   "id": "655130025ce06d58",
   "execution_count": 242
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training and test\n",
    "\n",
    "Reference:\n",
    "https://github.com/jhhuang96/ConvLSTM-PyTorch/blob/master/main.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c7b7fef2f95d4f0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                if self.verbose:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                return True\n",
    "        return False\n",
    "early_stopping = EarlyStopping()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T17:32:49.432241200Z",
     "start_time": "2024-11-26T17:32:49.394337400Z"
    }
   },
   "id": "fcbc8df0df53ab2d",
   "execution_count": 243
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, amsgrad=True\n",
    ")\n",
    "optimizer = optim.Adam(\n",
    "            model.parameters(), lr=learning_rate, amsgrad=True\n",
    "        )\n",
    "lambda1 = lambda epoch: 1.0**epoch\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "#                               factor=0.5,\n",
    "#                               patience=4,\n",
    "#                               verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T17:32:55.039601400Z",
     "start_time": "2024-11-26T17:32:54.946321600Z"
    }
   },
   "id": "97897e6f2ad9e6cd",
   "execution_count": 244
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[245], line 8\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m###################\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# train the model #\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m###################\u001B[39;00m\n\u001B[0;32m      7\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (sample, label) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[0;32m      9\u001B[0m     sample \u001B[38;5;241m=\u001B[39m sample\u001B[38;5;241m.\u001B[39mto(device\u001B[38;5;241m=\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat)\n\u001B[0;32m     10\u001B[0m     label \u001B[38;5;241m=\u001B[39m label\u001B[38;5;241m.\u001B[39mto(device\u001B[38;5;241m=\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\n",
      "File \u001B[1;32mD:\\Users\\dell\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mD:\\Users\\dell\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mD:\\Users\\dell\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcollate_fn(data)\n",
      "File \u001B[1;32mD:\\Users\\dell\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:277\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    216\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    217\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    218\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    276\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m collate(batch, collate_fn_map\u001B[38;5;241m=\u001B[39mdefault_collate_fn_map)\n",
      "File \u001B[1;32mD:\\Users\\dell\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    141\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 144\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    146\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\Users\\dell\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    141\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 144\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    146\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\Users\\dell\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:121\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    119\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    120\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m--> 121\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map)\n\u001B[0;32m    123\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m    124\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[1;32mD:\\Users\\dell\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    172\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    173\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[1;32m--> 174\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mstack(batch, \u001B[38;5;241m0\u001B[39m, out\u001B[38;5;241m=\u001B[39mout)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = [], []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    train_loss, val_loss = [], []\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for i, (sample, label) in enumerate(train_loader):\n",
    "        sample = sample.to(device=device, dtype=torch.float)\n",
    "        label = label.to(device=device, dtype=torch.long)\n",
    "\n",
    "        output = model(sample)  # x_encoded.shape=batch512,outchannel128,len13\n",
    "        loss = criterion(output, label)\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_losses.append(np.average(train_loss))\n",
    "\n",
    "    ######################\n",
    "    # validate the model #\n",
    "    ######################\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (sample, label) in enumerate(val_loader):\n",
    "            sample = sample.to(device=device, dtype=torch.float)\n",
    "            label = label.to(device=device, dtype=torch.long)\n",
    "    \n",
    "            output = model(sample)  # x_encoded.shape=batch512,outchannel128,len13\n",
    "            loss = criterion(output, label)\n",
    "            val_loss.append(loss.item())\n",
    "        val_losses.append(np.average(val_loss))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T17:32:56.222566600Z",
     "start_time": "2024-11-26T17:32:56.048601700Z"
    }
   },
   "id": "a18f514279c93576",
   "execution_count": 245
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_loss, test_losses = [], []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.eval()\n",
    "    true_labels, pred_labels = [], []\n",
    "    for i, (sample, label) in enumerate(test_loader):\n",
    "        sample = sample.to(device=device, dtype=torch.float)\n",
    "        label = label.to(device=device, dtype=torch.long)\n",
    "\n",
    "        output = model(sample)  # x_encoded.shape=batch512,outchannel128,len13\n",
    "        loss = criterion(output, label)\n",
    "        test_loss.append(loss.item())\n",
    "        \n",
    "        true_labels.append(label.detach().cpu().numpy())\n",
    "        pred_labels.append(output.detach().cpu().numpy())\n",
    "    \n",
    "    # break\n",
    "    test_losses.append(np.average(test_loss))\n",
    "    # break   \n",
    "    # Calculate F1 scores\n",
    "    y_true = np.concatenate(true_labels, axis=0)\n",
    "    y_prob = np.concatenate(pred_labels, axis=0)\n",
    "    \n",
    "    # Get the predicted class labels (argmax along the class dimension)\n",
    "    y_pred = np.argmax(y_prob, axis=1)  # output Shape: (batch_size, time_steps)\n",
    "\n",
    "    # Flatten the tensors for F1 score calculation\n",
    "    y_pred_flat = y_true.flatten()  # Flatten to 1D array\n",
    "    y_true_flat = y_pred.flatten()  # Flatten to 1D array\n",
    "\n",
    "    # Calculate F1 score (macro F1 score)\n",
    "    f1 = f1_score(y_true_flat, y_pred_flat, average='macro')\n",
    "\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    \n",
    "    # Check early stopping\n",
    "    if early_stopping(np.average(test_loss)):\n",
    "        print(\"Stopping at epoch %s.\"%str(epoch))\n",
    "        break\n",
    "    \n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T17:13:54.424336Z",
     "start_time": "2024-11-26T17:13:54.391946400Z"
    }
   },
   "id": "9b6e208b8a744644"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1afa4cecb0564bdf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
